<!DOCTYPE html>
<html>
<head>
    <meta charset="windows-1252">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Benjamin Scellier</title>
    <link href="styles.css" rel="stylesheet" type="text/css">
</head>
<body>
    <div class="container">
        <div class="header-section">
            <div class="header-text">
                <h1>Benjamin Scellier</h1>
                <p>benjamin at rain dot ai</p>
            </div>
            <div class="header-image">
                <img src="./img.jpg" alt="Profile Picture" width="200" height="200">
            </div>
        </div>
        <div>
            <p>I am a principal research scientist at  <a href="https://rain.ai/">Rain</a>.</p>
            <p>Before that, I was a resident at <a href="https://x.company/">Google X</a>, then a resident at Google Zurich, and then a postdoctoral researcher at the Department of Mathematics of ETH Zurich.</p>
            <p>I completed my PhD at <a href="https://mila.quebec/en/">Mila</a> (University of Montreal) under the supervision of <a href="https://yoshuabengio.org/">Yoshua Bengio</a>.</p>
        </div>
        <div align="center">
            <a href="https://scholar.google.com/citations?user=GWyeAskAAAAJ&hl=en">Google Scholar</a>
        </div>
        <div class="news-section">
            <h2>News</h2>
              <li> Equilibrium propagation is featured in <a href="https://www.quantamagazine.org/how-to-make-the-universe-think-for-us-20220531/"> Quanta Magazine</a></li>
              <li> New preprint: <a href="https://arxiv.org/abs/2205.15021">Agnostic physics-driven deep learning</a></li>
        </div>
        <div class="about-section">
            <h2>About me</h2>
            <p>My main research interests lie at the interface of deep learning, physics and neuroscience. I am more particularly interested in physics-based computation and learning.</p>
            <p>Much of my current research revolves around <a href="https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full">equilibrium propagation</a> (EP), a novel mathematical framework for gradient-descent-based machine learning. Compared to the more conventional framework based on automatic differentiation (i.e. "backpropagation"), the benefit of the EP framework is that inference and gradient computation are performed using the same physical laws. By suggesting a path to perform the desired computations (inference and learning) more efficiently, this framework may have implications for the design of novel hardware ("accelerators") for deep learning. For more information on EP, Chapter 2 of my <a href="https://papyrus.bib.umontreal.ca/xmlui/handle/1866/25593">PhD thesis</a> provides a (relatively recent) overview. See also <a href="eqprop/eqprop.html">these notes</a> on EP.
            </p>
        </div>
        <div>
            <h2>Invited Talks</h2>
            <li>March 2023. APS March Meeting. Session on "Learning in physical systems without neurons". "A mathematical framework of learning grounded in physical principles" (invited speaker)</li>
            <li>May 2021. ICLR 2021. Workshop on energy-based models. "A deep learning theory for neural networks grounded in physics" (invited speaker)</li>
            <li>August 2020. Neuromorphic Workshop Telluride. "Training nonlinear resistive networks with equilibrium propagation" (invited speaker)</li>
        </div>
    </div>
</body>
</html>